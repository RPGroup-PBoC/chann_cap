\section{Sarah Marzen}

So while I like the idea of the paper, I do have some comments that might occur
to the referees:

\begin{tcolorbox}
1)  Why is channel capacity the right thing to calculate?  Are these cells such
good engineers of their environment that they can tune P(c)?  Or are you just
saying that this is the maximum resolution these circuits could possibly have?
Either way, might want to justify channel capacity as the thing to calculate
earlier in the paper.  And if you're going to imply a fitness benefit to
additional bits, I think it's worth noting that maximum channel capacity
transducers are deterministic, though that seems to go against how the weak
binding gives higher channel capacity than strong binding (Fig 5)-- in fact,
that counterintuitive behavior with weakness of the binding site could use some
explanation.
\end{tcolorbox}

We thank Sarah for this very insightful comment. A couple of people have
brought this point to our attention, meaning that it is necessary to justify
better our information theoretic approach for this paper.

In this case there is no claim about cells being able to shape in any way the
distribution of environments $P(c)$. The reason we computed the channel
capacity was to understand what would the \textit{best case scenario} of this
simple genetic circuit performance look like. It is a view of the maximum
resolution with which cells could determine the environmental state given the
biophysical parameters encoded in their genetic circuit. It is true that in
principle we didn't need to compute the mutual information at channel capacity;
we could have instead chosen some arbitrary input function such as log-uniform.
But we argue that by showing the channel capacity we can learn about the upper
bound limits of what the physics of the system allow cells to do. Any other
arbitrary distribution could either be meaningful or not, but the fact that the
distribution at channel capacity has a clear mathematical interpretation
makes it an appealing example metric.

To reflect our argument more clearly in the main text we have modified the
following paragraph in the introduction:

``In this work we follow the same philosophy of theory-experiment dialogue used
to determine model parameters to predict from first principles the effect that
biophysical parameters such as transcription factor copy number and protein-DNA
affinity have on the information processing capacity of a simple genetic
circuit. Specifically, to predict the mutual information between an
extracellular chemical signal (input $c$) and the corresponding cellular
response in the form of protein expression (output $p$) (Eq. 1) we must compute
the input-output function $P(p \mid c)$. To do so, we use a
master-equation-based model to construct the protein copy number distribution as
a function of an extracellular inducer concentration for different combinations
of transcription factor copy numbers and binding sites. \textbf{Having these
input-output distributions allows us to compute the mutual information between
inputs and outputs $I(c; p)$ for any arbitrary input distribution $P(c)$. We opt
to compute the channel capacity, i.e. the maximum information that can be
processed by this gene regulatory architecture, defined as Eq. 1 maximized over
all possible input distributions $P(c)$. Doing so we can examine the physical
limits of what cells can do in terms of information processing by harboring
these genetic circuits}. All parameters used for our model were inferred from a
series of studies that span several experimental techniques \cite{Garcia2011c,
Jones2014a, Brewster2014, Razo-Mejia2018}, allowing us to perform parameter-free
predictions of this quantity \cite{Phillips2019}.''

Also in the discussion we have added the following lines to clarify to the
reader that in principle once we obtain a theoretical $P(p \mid c)$ that matches
the experimental one we could in principle compute the mutual information
$I(p ; c)$ for any arbitrary input distribution $P(c)$:

``... We compare these theoretical channel capacity predictions with
experimental determinations, finding that our minimal model is able to predict
with no free parameters this quantity. \textbf{In principle since our predicted
input-output distributions were in close agreement with experimental data we
could have chosen any arbitrary input distribution $P(c)$ and compute the mutual
information between input and outputs. The relevance of the channel capacity
comes from its interpretation as a metric of the physical limits of how precise
the inference that cells can make about what the state of the environment is
given this simple genetic circuit}.''

\begin{tcolorbox}
2)  To get a seemingly static distribution $P(p|c)$, you numerically integrate
$P_t(p|c)$ over time $t$.  If you're in steady state, then this procedure is
fine, and time $t$ shouldn't matter.  If you're dealing with a nonstationary
problem by averaging and getting a channel capacity from that average, then
well... it seems a little fishy to me, and I feel like you should be dealing
with the mutual information between input and output trajectories.
\end{tcolorbox}

We thank Sarah again for the excellent comment. The idea of dealing with
temporal trajectories of inputs and outputs, while extremely appealing to, is
out of the scope for this paper. In here we are assuming that if cells were to
experience changes in environmental state, these changes would take place at a
time scale much larger than the time to reach a new steady-state in gene
expression. Interestingly the steady-state that we refer to here is of a
different nature from what one would obtain by setting the time derivative to
zero - thereby we named it ``dynamical steady-state'' out of better options for
a name. The issue is that cells over a single cell cycle are changing since they
grow, duplicate their genomes and divide. But when look over multiple cell
cycles, the trajectory they follow is repeated systematically over and over
again. We have seen this experimentally; in our recent review when we tracked
cells over time we observed that on average cells at the beginning of their cell
cycle have the same level of expression \cite{Phillips2019}. So when we say that
to compute the distribution we average over time we mean that in order to
compare the experimental distributions - which were taken on unsynchronized
cells with no way of identifying cells with one vs two copies of the reporter
gene - we took the value of the input-output function at each time point, and
averaged it weighing each time point by the corresponding probability of finding
a cell at such point during their cell cycle.

In the text we added the following line:
``... We remind the reader that these time averages are done under a fixed
environmental state. It is the trajectory of cells over cell cycles what we need
to account for since all of our measurements were done on cells growing
exponentially under a constant inducer concentration.''

\begin{tcolorbox}
3)  I still think that when you say your theoretically predicted channel
capacity is the same as the experimentally measured channel capacity, you're
really saying that the theoretically predicted $P(p|c)$ is equivalent to the
measured $P(p|c)$.  Is that true?  And I have a bone to pick about the word
"measured"-- I wouldn't say that you've measured the channel capacity since
you've really measured $P(p|c)$ and calculated channel capacity from that (as in
you didn't directly measure channel capacity, but had to infer it from what you
did measure).
\end{tcolorbox}

We thank again Sarah for such clever and well thought comments. For us the hope
was that by sending the paper to experts in the field we could get a better
sense of how the professionals in the area think about the problem. So far each
and every comment has lived up and even exceeded our expectations.

In principle if we are able to construct an input-output function $P(p \mid c)$
that matches the experimental one we can compute the mutual information between
inputs and outputs $I(c; p)$ for any arbitrary distribution $P(c)$. We agree
that when we compare our theoretical inferences of the channel capacity with the
experimental values in some sense we are summarizing the comparison of several
input-output functions in a single number. So one could say that the channel
capacity reflects a comparison of input-output functions - which we explicitly
show in Fig. S17-S19. But as mentioned before, we think that the information
theoretic scope, and in particular the computation of the channel capacity puts
these results into a context of what performance could cells harboring these
systems have.

We also completely agree that we did not measure the channel capacity directly,
but rather inferred it after measuring the input-output functions. We have made
sure that nowhere in the text we give the impression that the channel capacity
is a quantity that can be directly measured.
