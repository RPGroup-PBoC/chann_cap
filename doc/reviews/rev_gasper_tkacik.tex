\section{Gasper Tkacik}

This is a beautiful connection between theory and the experiment!

I’m not commenting much on the bulk of the paper that constructs a biophysical
model except in so far it links to questions below.  I have some questions that
I feel could make the paper even stronger:

\begin{tcolorbox}
1) What are the optimal distributions recovered by BA algorithm? Don’t worry if
they are spiky: it is interesting to ask what is the total weight at low vs high
vs middle expression or so. Is there real evidence for a meaningful
“intermediate” state of induction, since you go over 1 bit? Maybe some playing
around needs to be done to figure out what are the key properties and not
numerical artifacts of these optimal solutions, or parametrize them in some
low-D space and optimize there. Our previous experiences have been that these
distributions are not very finely tuned, and only a few key statistics are
really important.
\end{tcolorbox}

We thank Gasper for such an excellent suggestion. As we learn our way through
this information-theoretic field this type of comments are really useful to
learn the relevant details in the field. \fref{revfig1_opt_dist} shows the
suggested inducer distributions $P(c)$ that the numerics of the Blahut-Arimoto
algorithm brought up. It looks indeed that for systems with more than one bit of
information the distribution is not bimodal, but rather trimodal with a relevant
mass of probability at a medium concentration. This mass is non-existent for the
two upper cases where there is less than one bit of information.

\begin{figure}[h!]
	\centering \includegraphics
  {../../fig/blahut_algorithm_channel_capacity/channcap_dist_fine_grid.pdf}
	\caption{\textbf{Input distributions at channel capacity.} Example optimal
	input distributions as numerically obtained with the Blahut-Arimoto
  algorithm.}
  \label{revfig1_opt_dist}
\end{figure}

We don't know exactly understand the point about exploring the lower dimensional
parametrization of these distributions. Is it a low parametrization of the
input-output distribution $P(p \mid c)$ such as approximating it as a Gaussian
distribution, or is it a low parametrization of the resulting input
distributions $P(c)$ at channel capacity as obtained by the Blahut-Arimoto
algorithm.

\begin{tcolorbox}
2) How does the capacity you report depend on various assumptions: (i) cell
cycle; (ii) non-gaussian noise distributions (order-by-order); (iii) multi-state
promoter; (iv) optimal vs log-flat distributions (= is Blahut-Arimoto really
important)? It would be great to have some sort of quantitative summary, all in
terms of the information, of how these (and other?) factors impact transmission,
like a table or a pie chart or something. This would also give us a metric on
how much various assumptions / mechanisms functionally matter, which many people
ask themselves about. You guys are in a perfect situation since you have a real
biophysical model that is properly calibrated. So something like: it is really
important to get the three state model, but not so important to take into
account the multiple copies of the DNA while replicating.
\end{tcolorbox}

We greatly appreciate this incredibly insightful comment. We agree that it is
worth highlighting the contribution of each of these points. It is hard to make
a single summary figure of how each of these elements contribute to the
conclusions. Nevertheless let us elaborate on each of these points:

\paragraph{(i) Cell Cycle.}
The inclusion of the gene copy number variability during the cell cycle and
the non-Poissoninan protein degradation is a key component for the correct
estimation of the input-output functions and as a consequence of the channel
capacity. This variability in gene copy number is an additional source of noise
that systematically decreaes the ability of the system to resolve different
inputs as shown in \fref{revfig2_channcap_compare}.

\begin{figure}[h!]
	\centering \includegraphics
  {../../fig/blahut_algorithm_channel_capacity/channcap_single_vs_multi.pdf}
	\caption{\textbf{Comparison of channel capacity predictions.} The
	multi-promoter model (solid line) vs. the single-promoter steady state model
	(dot-dashed line) shows systematic deviations. Assuming a single-promoter at
	steady state overestimates the channel capacity.}
  \label{revfig2_channcap_compare}
\end{figure}

\paragraph{(ii) Non-Gaussian noise distributions.}
For the construction of the probability distributions used in the main text
(Fig. 4) we utilized the first 6 moments of the protein distribution. But as
suggested by Gasper we could have simply used the first two moments for the
inference. To compare the results we would have obtained we built the maximum
entropy distributions using an increasing number of moments. We then computed
the Kullback-Leibler divergence $D_{KL}$ of the form
\begin{equation}
	D_{KL}(P_6(p \mid c) || P_i(p \mid c)) =
	\sum_p P_6(p \mid c) \log_2 {P_6(p \mid c) \over P_i(p \mid c)},
\end{equation}
where $P_i(p \mid c)$ is the maximum entropy distribution constructed with $i$
moments, $i \in \{2, 3, 4, 5, 6\}$. Since the Kullback-Leibler divergence
$D_{KL}(P || Q)$ can be interpreted as the amount of information lost by
assuming the incorrect distribution $Q$ when the correct distribution is $P$, we
used this metric as a way of how much information we would have lost by using
less constraints compared to the six moments used in the main text.
\fref{revfig3_dkl} shows this comparison for different operators and repressor
copy numbers. We can see from here that indeed using less moments would have
basically given the same result. Therefore the use of non-Gaussian noise is not
an important feature for our inferences.

\begin{figure}[h!]
	\centering \includegraphics
  {../../fig/MaxEnt_approx_joint/num_moments_vs_KL_div.pdf}
	\caption{\textbf{Measuring the loss of information by using different number
	of constraints.} The Kullback-Leibler divergence was computed between the
	maximum entropy distribution constructed using the first 6 moments of the
	distribution and a variable number of moments.}
  \label{revfig3_dkl}
\end{figure}

\paragraph{(iii) Multi-state promoter.}
This particular point is something that we are still exploring from a
theoretical perspective. We have shown that in order to capture the single-
molecule mRNA FISH data a single-state promoter wouldn't suffice. This model
predicts a Poisson distribution as the steady-state and the data definitely
shows super Poissonian noise. Given the bursty nature of gene expression we opt
to use a two-state promoter where the states reflect effective
transcriptionally ``active'' and ``inactive'' states. We are currently exploring
alternative formulations of this model to turn it into a single state with a
geometrically distributed burst-size, but that is still work in progress.

\paragraph{(iv) Optimal vs Log-flat Distributions.}
The relevance of having use the Blahut-Arimoto algorithm to predict the maximum
mutual information between input and outputs was just to understand the best-
case scenario. In reality we could have used any arbitrary input distribution
$P(c)$ as long as our theoretical input-output distribution $P(p \mid c)$ are
able to reproduce the experimental distributions. As we show in Fig. S17-S19 the
maximum entropy distributions we constructed are strikingly similar to the
experimental distributions.

We still believe that the channel capacity puts the results in a context. By
reporting the upper bound in performance of these genetic circuits we can start
to dissect how different molecular parameters such as repressor-DNA binding
affinity or repressor copy number affect the ability of this genetic circuit to
extract information from the enviromental state.

\begin{tcolorbox}
3) The fact that high repressor copy number turns down the transmission is
interesting - we had to put in a “cost” to regulation explicitly to get an
optimum in transmission, otherwise in our theory with Bill and Alex, more TF is
always good. Maybe it is an interesting comment to see more precisely why this
happens. Do you titrate out the inducer with excess TFs? Or is it that there is
some residual binding of the TF even in the inactive form, and it thus represses
still, which gets important at high repressor copy number? In general, it is an
interesting point since we have been speculating about how excess TFs can mess
up regulation (https://www.nature.com/articles/ncomms12307), and have been doing
some followup work on this.
\end{tcolorbox}

We agree that it is very interesting to see that optimal combination of
repressor copy number and repressor-DNA binding energy. The reason why this
happens is similar to the second suggestion. The inducer is always in excess
compared to the TF copy number, so there is no inducer titration effect. The
reason for this behavior is that the Monod-Wyman-Changeux model used to model
the allosteric nature of the repressor shows that the repressor never has a
probability 1 of being in the inactive state given the inferred parameters
\cite{Razo-Mejia2018}. That means that even if the culture is saturated the
with inducer, there would be an existing fraction of TFs that are still in the
active state able to repress. Therefore having excess of TFs means that cells
are always repressed regardless of the inducer concentration. This results in
all distributions starting to overlap at low expression levels - losing
information processing capacity.

As suggested we added this to the discussion. The discussion paragraph about the
channel capacity now looks as follows:

``... The relevance of the channel capacity comes from its interpretation as a
metric of the limits of how precise the inference that cells can make about what
the state of the environment is given this simple genetic circuit. Our model
makes non-trivial predictions such as the existence of an optimal repressor copy
number for a given repressor-DNA binding energy (See Fig. 5(A)). We
note that this differs from previous theoretical results since this optimal
combination does not come from adding a cost term for the regulation
\cite{Tkacik2011}. This is a consequence of the parameters inferred in
\cite{Razo-Mejia2018} for the allosteric repressor never allowing all repressors
to go into the inactive (non-DNA binding) state. That means that even at
saturating concentrations of inducer, as the number of repressors increases, a
significant number of them are still able to bind to the promoter. This causes
all of the input-output functions to be biased towards low expression levels,
decreasing the amount of information that the circuit is able to process.''

\begin{tcolorbox}
4) What I find extremely cool is that you have “experimentally instantiated” the
kind of optimization w.r.t. molecular parameters that we studied with Bill and
Aleks and Sokolowski in the PRE series of papers (Part I, II, III, IV). For
instance, Figure 4 clearly suggests that there is an optimal combination of
repressors/cell and the binding energy (at a given copy number, this would give
you an optimum for the binding energy). This is an ab-initio prediction for the
energy, which could be contrasted in the WT system to the actual value. I’m not
sure that in this paper there is scope to do this, but maybe it is nice to
connect it to optimization approaches and show why also “mutual information”
might be a relevant quantity: it could have high predictive power if it
correctly predicts the molecular properties of regulation. Sometimes, even the
existence of the optimum of information w.r.t. some molecular parameter is
surprising (at least to us it was in the first years) since it arises from the
non-trivial interaction between the shape of mean induction, dynamic range, and
the noise response.
\end{tcolorbox}

We really appreciate this insightful and thoughtful suggestion. We agree that it
is interesting that there seems to be an optimal combination of a particular
binding site and a particular repressor copy number that maximizes the amount of
information processed. Nevertheless we are not in the position to make any
meaningful biological conclusion with respect to the wild-type system. We have
worked with an oversimplified version of the {\it lac} operon where elements
such as multiple binding sites for the repressor that allow it to form DNA
loops, an active transporter to import the inducer, and an activator binding
site have been removed. Therefore any conclusions we could make about the
optimality of this simplified system wouldn't reflect the case of the wild-type
construct.

Having said that, this is a suggestion that we will keep on the back of our
minds. As we move forward with these ideas, it is interesting to think from an
experimental evolution mindset how these optimality ideas could be tested in a
controlled lab setup.

\begin{tcolorbox}
5) Crazy question: if you set up an experiment where the bugs have an interest
to express tightly in response to the environment, could you see evolutionary
dynamics towards information maximum? Or examine how they evolve towards higher
information? Long time ago we wanted to take this approach with Sergey
Kryazhimskiy (at UCSD). I think it is still a thrilling prospect for an
evolutionary experiment… this is something you seem to suggest in the last
sentence, and I would be very curious on how you’d plan to do it (we had some
more concrete ideas, in that case with yeast and temperature, but this would be
great to maybe think about together).
\end{tcolorbox}

The real motivation behind this project has always been exactly this. I (Manuel)
was profoundly inspired by the Taylor, Tishby and Bialek's ArXiv paper on
Information and Fitness, and we wanted to build a simple system to test these
ideas. We are finally in the position to do this kind of experimental evolution
approach. On top of that we recently had Michael Lassig visiting us over here,
so the hype for this biophysics-evolution is at its maximum right now.

\begin{tcolorbox}
In general, I think points (2) and (4) can be highlighted in the Discussion as a
motivation for mutual information as an actually interesting / relevant metric.
First, it can “measure” how important each actual physical effect, or our
approximation in the computations, is to the function of a regulatory circuit
(2). Second, it can predict optimal molecular / systems parameters that can be
compared to real measurements (4). I know these are obvious in hindsight, but
for a broader field, one could highlight again.
\end{tcolorbox}
